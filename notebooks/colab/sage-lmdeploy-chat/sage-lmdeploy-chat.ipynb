{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YALmXnwCG1pQ","collapsed":true},"outputs":[],"source":["!pip install lmdeploy"]},{"cell_type":"code","source":[],"metadata":{"id":"ROlSxXy9zRvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================================\n","# Mount Google Drive\n","# ===========================================\n","\n","from google.colab import drive\n","import os\n","\n","# --- Mount Google Drive ---\n","drive.mount('/content/drive')\n","\n","print(f\"✅ Google Drive mounted successfully.\")"],"metadata":{"id":"naoKad7pzR3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xsWms51EzSD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Eg1j1dKPKNr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================================\n","# Load zrok token from ZIP in datasets\n","# ===========================================\n","\n","import os, sys, zipfile\n","\n","# Path to the ZIP containing .zrok_api_token\n","drive_zip = \"/content/drive/MyDrive/datasets/sage-zrok-token.zip\"\n","token_target = \"/root/.zrok_api_key\"\n","\n","# Extract the token if it doesn't already exist\n","if os.path.exists(drive_zip):\n","    if not os.path.exists(token_target):\n","        print(\"📦 Extracting .zrok_api_key from Drive ZIP...\")\n","        with zipfile.ZipFile(drive_zip, 'r') as zip_ref:\n","            zip_ref.extract('.zrok_api_key', '/root/')\n","        print(f\"✅ Token extracted to {token_target}\")\n","    else:\n","        print(f\"✅ Token already exists at {token_target}\")\n","else:\n","    print(f\"⚠️ Token archive not found: {drive_zip}\")\n","\n","# Load token into variable\n","zrok_token = None\n","if os.path.isfile(token_target):\n","    with open(token_target, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n","        zrok_token = f.read().strip()\n","\n","if not zrok_token:\n","    print(f\"❌ zrok token not found or empty at {token_target}\")\n","    sys.exit(1)\n","else:\n","    print(\"✅ zrok token loaded and ready for use.\")"],"metadata":{"id":"gBH9NSUszSOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8nJYQD4pzShH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bUGsRrd--2XT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pipeline loading (GDrive fallback or HF repo)\n","# This cell must produce a `pipe` object that will be used by the API.\n","\n","from pathlib import Path\n","import os\n","import time\n","import shutil\n","from lmdeploy import pipeline, TurbomindEngineConfig\n","\n","# =======================\n","# Model selection\n","# =======================\n","MODEL_TAG = \"qwen/Qwen2.5-Coder-14B-Instruct-AWQ\"\n","MODEL_LOCAL_DIR = f\"/content/drive/MyDrive/models/{MODEL_TAG}\"\n","USE_LOCAL = os.path.isdir(MODEL_LOCAL_DIR)\n","\n","\n","print(\"Using local model dir:\", USE_LOCAL, MODEL_LOCAL_DIR if USE_LOCAL else MODEL_TAG)\n","\n","# =======================\n","# Engine config\n","# =======================\n","engine_config = TurbomindEngineConfig(model_format='awq', cache_max_entry_count=0.3)\n","\n","# =======================\n","# Load pipeline\n","# =======================\n","start = time.time()\n","\n","try:\n","    if USE_LOCAL:\n","        print(f\"📂 Loading from local cache: {MODEL_LOCAL_DIR}\")\n","        pipe = pipeline(MODEL_LOCAL_DIR, backend_config=engine_config)\n","    else:\n","        print(f\"🌐 Loading from Hugging Face: {MODEL_TAG}\")\n","        pipe = pipeline(MODEL_TAG, backend_config=engine_config)\n","\n","        # If Drive is mounted, cache the snapshot folder only\n","        drive_root = \"/content/drive\"\n","        hf_cache_dir = f\"/root/.cache/huggingface/hub/models--{MODEL_TAG.replace('/', '--')}\"\n","        snapshots_dir = os.path.join(hf_cache_dir, \"snapshots\")\n","\n","        if os.path.ismount(drive_root) and os.path.exists(snapshots_dir):\n","            print(\"💾 Caching model snapshot to Google Drive...\")\n","\n","\n","\n","            # Ensure parent folder exists\n","            os.makedirs(os.path.dirname(MODEL_LOCAL_DIR), exist_ok=True)\n","\n","            # Copy the snapshot folder\n","            snapshot_hash = os.listdir(snapshots_dir)[0]\n","            snapshot_path = os.path.join(snapshots_dir, snapshot_hash)\n","            shutil.copytree(snapshot_path, MODEL_LOCAL_DIR)\n","            print(f\"✅ Model cached at {MODEL_LOCAL_DIR}\")\n","        else:\n","            print(\"❌ Snapshots folder does not exist or Drive is not mounted.\")\n","\n","except Exception as e:\n","    print(f\"❌ Failed to load model: {e}\")\n","\n","print(f\"⏱️ Pipeline ready in {time.time() - start:.1f}s\")"],"metadata":{"id":"W1hmxw2j_u4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RTBIoshV9ATy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AmhaKtdVf_VB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# internal helper method(s) - core generation function\n","# This is the function your API will call internally.\n","# It is NOT the FastAPI endpoint. Keep the internals here (single concern).\n","\n","from typing import List, Dict, Optional\n","\n","def render_messages_to_prompt(messages: List[Dict[str,str]]) -> str:\n","    \"\"\"\n","    Convert OpenAI-style messages list into a single prompt string for the model.\n","    You can adapt this to your preferred chat template (system/user/assistant tokens).\n","    \"\"\"\n","    parts = []\n","    for m in messages:\n","        role = m.get(\"role\", \"user\")\n","        content = m.get(\"content\", \"\")\n","        parts.append(f\"{role}: {content}\")\n","    parts.append(\"assistant:\")\n","    return \"\\n\".join(parts)\n","\n","def generate_completion(pipe, messages: List[Dict[str,str]],\n","                        temperature: float = 0.7, max_tokens: int = 512) -> str:\n","    \"\"\"\n","    Run the model pipeline with the prepared prompt and return generated text.\n","    - pipe: lmdeploy pipeline object\n","    - messages: list of {\"role\": \"...\", \"content\": \"...\"}\n","    \"\"\"\n","    prompt = render_messages_to_prompt(messages)\n","    # LMDeploy pipeline may accept keyword args; pass them if supported\n","    try:\n","        # Common LMDeploy pipeline usage: out = pipe(prompt); sometimes pipe returns dict or object\n","        out = pipe(prompt, max_tokens=max_tokens, temperature=temperature)\n","    except TypeError:\n","        # If pipe doesn't accept kwargs, call without them\n","        out = pipe(prompt)\n","\n","    # Normalize output to string\n","    text = None\n","    if hasattr(out, \"text\"):\n","        text = out.text\n","    elif isinstance(out, dict) and \"text\" in out:\n","        text = out[\"text\"]\n","    elif isinstance(out, (list, tuple)) and len(out) and isinstance(out[0], dict) and \"text\" in out[0]:\n","        text = out[0][\"text\"]\n","    elif isinstance(out, str):\n","        text = out\n","    else:\n","        # Fallback: stringify\n","        text = str(out)\n","\n","    return text"],"metadata":{"id":"iJpmm5TZgACQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"V4HIcIF53n-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"27zombZ7Se8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FastAPI app init (OpenAI-compatible endpoints) with API key auth\n","# Hybrid API: uses your generate_completion() helper internally\n","# Matches OpenAI ChatCompletions schema (non-streaming)\n","\n","from fastapi import FastAPI, Header, HTTPException, Request\n","from pydantic import BaseModel\n","from typing import List, Optional\n","import json\n","\n","# -------------------------\n","# Server-side API key (static for testing)\n","# -------------------------\n","API_KEY = \"12345\"  # client should send this in Authorization: Bearer <key>\n","\n","# -------------------------\n","# FastAPI app & schemas\n","# -------------------------\n","app = FastAPI(title=\"LMDeploy OpenAI-Compatible API (local)\")\n","\n","# -------------------------\n","# Logging middleware (for debugging zrok)\n","# -------------------------\n","@app.middleware(\"http\")\n","async def log_requests(request: Request, call_next):\n","    print(f\"[FastAPI] Incoming request: {request.method} {request.url}\")\n","    response = await call_next(request)\n","    return response\n","\n","# -------------------------\n","# Root / fallback route\n","# -------------------------\n","@app.get(\"/\")\n","def root():\n","    return {\"status\": \"ok\", \"message\": \"FastAPI server is running\"}\n","\n","# -------------------------\n","# Models & request schemas\n","# -------------------------\n","class Message(BaseModel):\n","    role: str\n","    content: str\n","\n","class ChatRequest(BaseModel):\n","    model: Optional[str] = None\n","    messages: List[Message]\n","    temperature: Optional[float] = 0.7\n","    max_tokens: Optional[int] = 512\n","\n","# -------------------------\n","# Endpoints\n","# -------------------------\n","@app.get(\"/v1/models\")\n","def list_models():\n","    return {\"data\": [{\"id\": MODEL_TAG if not USE_LOCAL else MODEL_LOCAL_DIR, \"object\": \"model\"}]}\n","\n","@app.get(\"/health\")\n","def health():\n","    return {\"status\": \"ok\"}\n","\n","@app.post(\"/v1/chat/completions\")\n","async def chat_completions(req: ChatRequest, authorization: Optional[str] = Header(None)):\n","    # ----- Authorization check (optional) -----\n","    # if API_KEY:\n","    #     if not authorization or authorization.strip() != f\"Bearer {API_KEY}\":\n","    #         raise HTTPException(status_code=401, detail=\"Unauthorized\")\n","\n","    # Convert Pydantic Messages to list-of-dicts\n","    messages = [{\"role\": m.role, \"content\": m.content} for m in req.messages]\n","\n","    # Call local generation helper\n","    try:\n","        text = generate_completion(pipe, messages,\n","                                   temperature=req.temperature or 0.7,\n","                                   max_tokens=req.max_tokens or 512)\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=str(e))\n","\n","    # Build OpenAI-compatible response\n","    resp = {\n","        \"id\": \"chatcmpl-local-001\",\n","        \"object\": \"chat.completion\",\n","        \"created\": int(__import__(\"time\").time()),\n","        \"model\": MODEL_TAG if not USE_LOCAL else MODEL_LOCAL_DIR,\n","        \"choices\": [\n","            {\n","                \"index\": 0,\n","                \"message\": {\"role\": \"assistant\", \"content\": text},\n","                \"finish_reason\": \"stop\"\n","            }\n","        ],\n","        \"usage\": {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0}\n","    }\n","    return resp"],"metadata":{"id":"iqmKB7pZSfnl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D9BTtTMuSf7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HE3F1TqFgBhX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download zrok v1.1.3 (latest)\n","!wget https://github.com/openziti/zrok/releases/download/v1.1.3/zrok_1.1.3_linux_amd64.tar.gz\n","!tar -xzf zrok_1.1.3_linux_amd64.tar.gz\n","!chmod +x zrok"],"metadata":{"id":"QcCFiDu5gB6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Enable (automatic migration from 0.4)\n","!./zrok enable --headless \"$zrok_token\"\n","\n","# Use the agent for better process management\n","#!./zrok agent start &\n","#!./zrok share public localhost:8000 --headless"],"metadata":{"id":"hRd-tZy2guhs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!./zrok disable"],"metadata":{"id":"Ux__J8PtgCGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9DDZ0E9SRZyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"w_2eK3BQRaRv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import uvicorn\n","import threading\n","\n","def run_uvicorn():\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n","\n","# Start in background thread\n","threading.Thread(target=run_uvicorn, daemon=True).start()"],"metadata":{"id":"71cpVN5_gCSi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import subprocess\n","import re\n","import time\n","\n","def start_zrok_tunnel(port=8000):\n","    # Start the tunnel\n","    process = subprocess.Popen([\n","        \"./zrok\", \"share\", \"public\", f\"localhost:{port}\", \"--headless\"\n","    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n","\n","    # Give it a moment to start\n","    time.sleep(3)\n","\n","    # Check agent status to get the URL\n","    status_process = subprocess.run([\n","        \"./zrok\", \"agent\", \"status\"\n","    ], capture_output=True, text=True)\n","\n","    print(\"Agent Status:\")\n","    print(status_process.stdout)\n","\n","    return process\n","\n","# Start the tunnel\n","tunnel_process = start_zrok_tunnel(8000)\n","print(\"Zrok tunnel started! Check the agent status above for your public URL.\")"],"metadata":{"id":"AyMttY6fgCfC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./zrok overview"],"metadata":{"id":"bsBAmZOmhNZh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","print(\"Server and zrok tunnel are running. Keeping the notebook alive...\")\n","\n","try:\n","    while True:\n","        time.sleep(60)\n","except KeyboardInterrupt:\n","    print(\"Shutting down.\")"],"metadata":{"id":"UXW3-cduhNjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3neg9eU8fKpG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_NYkJnn1IoPp"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1mkWUuCTFVfo7IS4QsZXSLTvXton9RZwu","timestamp":1761733236928},{"file_id":"1Dh-YlSwg78ZO3AlleO441NF_QP2shs95","timestamp":1761700044276}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}