{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13470201,"sourceType":"datasetVersion","datasetId":8550945},{"sourceId":13470212,"sourceType":"datasetVersion","datasetId":8550952}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys\n\np = \"/kaggle/input/sage-zrok-token/.zrok_api_key\"\nzrok_token = None\n\nif os.path.isfile(p):\n    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        zrok_token = f.read().strip()\n\nif not zrok_token:\n    print(\"❌ Token not found or empty:\", p)\n    sys.exit(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\nprint(\"Setting up models...\")\n\n# --- Copy Dreamshaper-7 model if not already present ---\nsource = \"/kaggle/input/dreamshaper-7\"\ndest = \"/kaggle/working/Lykon/dreamshaper-7\"\n\nif os.path.exists(dest):\n    print(f\"✓ Dreamshaper-7 already exists at {dest}, skipping copy\")\nelse:\n    print(f\"  Copying Dreamshaper-7...\")\n    shutil.copytree(source, dest)\n    print(f\"  ✓ Copied to {dest}\")\n\n# --- Copy LCM UNet if not already present ---\nlcm_source = \"/kaggle/input/lcm-dreamshaper-v7-unet\"\nlcm_dest = \"/kaggle/working/SimianLuo/LCM_Dreamshaper_v7\"\n\nif os.path.exists(lcm_dest):\n    print(f\"✓ LCM UNet already exists at {lcm_dest}, skipping copy\")\nelse:\n    print(f\"  Copying LCM UNet...\")\n    shutil.copytree(lcm_source, lcm_dest)\n    print(f\"  ✓ Copied to {lcm_dest}\")\n\nprint(\"✅ All models ready!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# -----------------------------\n# TXT2IMG + LCM pipeline\n# -----------------------------\n\nimport os\nimport torch\nfrom diffusers import DiffusionPipeline, UNet2DConditionModel, LCMScheduler\n\n# --- Device ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# LCM UNet path\nlocal_lcm_path = \"/kaggle/working/SimianLuo/LCM_Dreamshaper_v7\"\n\n# Download and save LCM UNet if not exists\nif os.path.exists(local_lcm_path):\n    print(f\"✓ LCM UNet already exists at {local_lcm_path}\")\n    unet = UNet2DConditionModel.from_pretrained(\n        local_lcm_path,\n        subfolder=\"unet\",\n        torch_dtype=torch.float16,\n    )\nelse:\n    print(f\"Downloading LCM UNet...\")\n    unet = UNet2DConditionModel.from_pretrained(\n        \"SimianLuo/LCM_Dreamshaper_v7\",\n        subfolder=\"unet\",\n        torch_dtype=torch.float16,\n    )\n    # Save it locally\n    os.makedirs(local_lcm_path, exist_ok=True)\n    unet.save_pretrained(os.path.join(local_lcm_path, \"unet\"))\n    print(f\"✓ LCM UNet saved to {local_lcm_path}\")\n\n\n\n# --- Model / LoRA paths & IDs ---\nMODEL_ID = \"Lykon/dreamshaper-7\"\nlocal_model_path = \"/kaggle/working/Lykon/dreamshaper-7\"  # change to your working dir\n#LORA_ID = \"latent-consistency/lcm-lora-sdxl\"\n#local_lora_path = \"/kaggle/working/latent-consistency/lcm-lora-sdxl\"         # change to your working dir\n\n# --- Load or download Stable Diffusion (DiffusionPipeline) ---\nif os.path.exists(local_model_path):\n    pipe_txt2img = DiffusionPipeline.from_pretrained(\n        local_model_path,\n        unet=unet,\n        #variant=\"fp16\",\n        torch_dtype=torch.float16\n    ).to(device)\nelse:\n    pipe_txt2img = DiffusionPipeline.from_pretrained(\n        MODEL_ID,\n        unet=unet,\n        torch_dtype=torch.float16,\n        variant=\"fp16\",\n    ).to(device)\n\n\n\n\n    # save local copy\n    try:\n        pipe_txt2img.save_pretrained(local_model_path)\n    except Exception as e:\n        print(f\"Warning: failed to save pipeline locally to {local_model_path}: {e}\")\n\n# set scheduler\npipe_txt2img.scheduler = LCMScheduler.from_config(pipe_txt2img.scheduler.config)\n\n\n\n\n# --- Load LCM-LoRA (prefer local copy) ---\n#if os.path.exists(local_lora_path):\n#    pipe_txt2img.load_lora_weights(local_lora_path)\n#else:\n#    pipe_txt2img.load_lora_weights(LORA_ID)\n#\n\n\npipe_txt2img.safety_checker = lambda images, clip_input=None: (images, [False] * len(images))\n\n\n\n# ensure pipeline on device\n#pipe_txt2img = pipe_txt2img.to(device)","metadata":{"id":"6U5TrtW7hksb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"Q4dLMo2FlOrI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"KQRP8RUElO0r","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"ekaJZWL91sC3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ----------------------\n# IMG2IMG + LCM pipeline\n# ----------------------\n\nimport torch\nfrom diffusers import AutoPipelineForImage2Image, UNet2DConditionModel, LCMScheduler\nfrom diffusers.utils import load_image\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"SimianLuo/LCM_Dreamshaper_v7\",\n    subfolder=\"unet\",\n    torch_dtype=torch.float16,\n)\n\npipe_img2img = AutoPipelineForImage2Image.from_pretrained(\n    \"Lykon/dreamshaper-7\",\n    unet=unet,\n    torch_dtype=torch.float16,\n    #variant=\"fp16\",\n).to(\"cuda\")\npipe_img2img.scheduler = LCMScheduler.from_config(pipe_img2img.scheduler.config)\n\npipe_img2img.safety_checker = lambda images, clip_input=None: (images, [False] * len(images))","metadata":{"id":"b8XMpV7W1sGU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"eflsIfQ_1pvY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"3rSY9im-1p3U","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"tRT5sgp4mwOC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# -----------------------------\n# METHOD text-to-image generator\n# (returns PIL.Image for easy FastAPI integration)\n# -----------------------------\n\nfrom datetime import datetime\nfrom IPython.display import display\nimport torch\nimport io\nimport os\n\ndef generate_txt2img(\n    pipe_txt2img,\n    prompt,\n    negative_prompt=None,\n    seed=None,\n    num_inference_steps=4,\n    guidance_scale=1.0,\n    height=768,\n    width=768,\n    device=None\n):\n    \"\"\"\n    Generate an image with `pipe_txt2img` and return a PIL.Image (does not save by default).\n    If save=True it will write a PNG to disk but still return the PIL.Image.\n    If return_metadata=True it returns (image, metadata_dict).\n    \"\"\"\n    # device detection\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # choose seed\n    if seed is None:\n        seed = int(torch.randint(0, 2**31 - 1, (1,)).item())\n\n    # create generator for the chosen device (fallback to CPU if unsupported)\n    try:\n        gen = torch.Generator(device=device).manual_seed(seed)\n    except Exception:\n        gen = torch.Generator().manual_seed(seed)\n\n    # build call kwargs\n    call_kwargs = {\n        \"prompt\": prompt,\n        \"num_inference_steps\": int(num_inference_steps),\n        \"guidance_scale\": float(guidance_scale),\n        \"generator\": gen,\n    }\n    if negative_prompt is not None:\n        call_kwargs[\"negative_prompt\"] = negative_prompt\n    if height is not None:\n        call_kwargs[\"height\"] = int(height)\n    if width is not None:\n        call_kwargs[\"width\"] = int(width)\n\n\n    # call the pipeline\n    pipeline_output = pipe_txt2img(**call_kwargs)\n\n    # extract image (typical diffusers output)\n    try:\n        image = pipeline_output.images[0]\n    except Exception:\n        image = pipeline_output[0]\n\n\n    return image","metadata":{"id":"CD7PUqlaYs3M","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"UpmYywyLmwVI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"Feeq_9AclPOi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------- #\n# Reusable img2img generator (SDXL + LCM)\n# ------------------------------------- #\nimport torch\nimport random\n\ndef generate_img2img(\n    pipe_img2img,\n    prompt,\n    init_image,\n    negative_prompt=None,\n    strength=0.8,\n    guidance_scale=1.0,\n    num_inference_steps=4,\n    seed=None,\n    dtype=torch.float16,\n    device=None\n):\n    \"\"\"\n    Generate an image with `pipe_img2img` and return a PIL.Image (does not save by default).\n    Designed for SDXL + LCM LoRA pipelines.\n    \"\"\"\n\n    # --- Device detection ---\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # --- Seed handling ---\n    if seed is None:\n        seed = int(torch.randint(0, 2**31 - 1, (1,)).item())\n\n    try:\n        generator = torch.Generator(device=device).manual_seed(seed)\n    except Exception:\n        generator = torch.Generator().manual_seed(seed)\n\n    # --- Build call kwargs ---\n    call_kwargs = {\n        \"prompt\": prompt,\n        \"image\": init_image,\n        \"strength\": float(strength),\n        \"guidance_scale\": float(guidance_scale),\n        \"num_inference_steps\": int(num_inference_steps),\n        \"generator\": generator,\n    }\n    if negative_prompt is not None:\n        call_kwargs[\"negative_prompt\"] = negative_prompt\n\n    # --- Run pipeline ---\n    with torch.inference_mode(), torch.autocast(device, dtype=dtype):\n        pipeline_output = pipe_img2img(**call_kwargs)\n\n    # --- Extract result ---\n    try:\n        image = pipeline_output.images[0]\n    except Exception:\n        image = pipeline_output[0]\n\n    return image","metadata":{"id":"Ho5lX0jElPUG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"I7p5hf2ohk3z","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"qShlMxGxoH8C","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"nOacxIfIYtTn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------\n# FastAPI app init\n# --------------------------\n\nfrom fastapi import FastAPI\nimport nest_asyncio\nimport uvicorn\n\n# Assume generate_txt2img, generate_img2img, pipe_txt2img, pipe_img2img are already defined\napp = FastAPI()\nnest_asyncio.apply()  # allow running uvicorn in Colab\n#print(\"FastAPI app initialized.\")","metadata":{"id":"NW9bWcz7Ytan","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"iQp-O3NNYtxG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"V08PrK5sNpyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ---------------------------\n# Unified T2I/I2I endpoint /generate (SDXL + LCM LoRA)\n# ---------------------------\n\nfrom fastapi import Form, UploadFile, File\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom io import BytesIO\nfrom PIL import Image\n\n@app.post(\"/generate\")\nasync def generate(\n    prompt: str = Form(...),\n    negative_prompt: str = Form(None),\n    height: int = Form(768),\n    width: int = Form(768),\n    num_inference_steps: int = Form(4),       # LCM default low steps\n    guidance_scale: float = Form(1.0),        # LCM default guidance\n    seed: int = Form(None),\n\n    # img2img-only params\n    strength: float = Form(0.8),\n    init_image: UploadFile = File(None),      # optional: if present → img2img\n):\n    \"\"\"Unified endpoint: txt2img if no init_image, img2img if init_image provided.\"\"\"\n\n    # --- Img2img path ---\n    if init_image is not None:\n        try:\n            init_img = Image.open(BytesIO(await init_image.read())).convert(\"RGB\")\n        except Exception as e:\n            return JSONResponse(\n                status_code=400,\n                content={\"error\": f\"Failed to read init_image: {str(e)}\"},\n            )\n\n        try:\n            generated_image = generate_img2img(\n                pipe_img2img,\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                init_image=init_img,\n                strength=strength,\n                guidance_scale=guidance_scale,\n                num_inference_steps=num_inference_steps,\n                seed=seed,\n            )\n        except Exception as e:\n            return JSONResponse(status_code=500, content={\"error\": str(e)})\n\n    # --- Txt2img path ---\n    else:\n        try:\n            generated_image = generate_txt2img(\n                pipe_txt2img,\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                height=height,\n                width=width,\n                num_inference_steps=num_inference_steps,\n                guidance_scale=guidance_scale,\n                seed=seed,\n            )\n        except Exception as e:\n            return JSONResponse(status_code=500, content={\"error\": str(e)})\n\n    # --- Return PNG stream ---\n    buffer = BytesIO()\n    generated_image.save(buffer, format=\"PNG\")\n    buffer.seek(0)\n    return StreamingResponse(buffer, media_type=\"image/png\")","metadata":{"id":"0CizW9kdpPEX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"tEUishJUpPMU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"J0TcZRUppPQY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"DQKtR6TxO-Ts","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download zrok v1.1.3 (latest)\n!wget https://github.com/openziti/zrok/releases/download/v1.1.3/zrok_1.1.3_linux_amd64.tar.gz\n!tar -xzf zrok_1.1.3_linux_amd64.tar.gz\n!chmod +x zrok","metadata":{"id":"eO81fsViO-Ye","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enable (automatic migration from 0.4)\n!./zrok enable --headless \"$zrok_token\"\n\n# Use the agent for better process management\n#!./zrok agent start &\n#!./zrok share public localhost:8000 --headless","metadata":{"id":"3XGpCMjFO-bc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!./zrok disable","metadata":{"id":"qvl2xM3pO-d0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import uvicorn\nimport threading\n\ndef run_uvicorn():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n\n# Start in background thread\nthreading.Thread(target=run_uvicorn, daemon=True).start()","metadata":{"id":"pHH-93euO-ri","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import subprocess\nimport re\nimport time\n\ndef start_zrok_tunnel(port=8000):\n    # Start the tunnel\n    process = subprocess.Popen([\n        \"./zrok\", \"share\", \"public\", f\"localhost:{port}\", \"--headless\"\n    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\n    # Give it a moment to start\n    time.sleep(3)\n\n    # Check agent status to get the URL\n    status_process = subprocess.run([\n        \"./zrok\", \"agent\", \"status\"\n    ], capture_output=True, text=True)\n\n    print(\"Agent Status:\")\n    print(status_process.stdout)\n\n    return process\n\n# Start the tunnel\ntunnel_process = start_zrok_tunnel(8000)\nprint(\"Zrok tunnel started! Check the agent status above for your public URL.\")","metadata":{"id":"jWt8d9UqO-vU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"k5vT7HsOQIA7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!./zrok overview","metadata":{"id":"tAXFNNnpNqf7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"qqAO1R_kQIGK","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"tZkjBnCpjyRF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nprint(\"Server and zrok tunnel are running. Keeping the notebook alive...\")\n\ntry:\n    while True:\n        time.sleep(60)\nexcept KeyboardInterrupt:\n    print(\"Shutting down.\")","metadata":{"id":"_vupHXeDjyVf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"2cHyLENtQIVw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"XsF3SWhNkqN_","trusted":true},"outputs":[],"execution_count":null}]}