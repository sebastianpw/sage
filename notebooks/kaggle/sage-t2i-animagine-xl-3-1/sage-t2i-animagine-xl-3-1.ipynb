{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNGltucRSnJvWnVawYtCuWQ"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13470697,"sourceType":"datasetVersion","datasetId":8551256},{"sourceId":13482155,"sourceType":"datasetVersion","datasetId":8559542}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys\n\np = \"/kaggle/input/sage-zrok-token/.zrok_api_key\"\nzrok_token = None\n\nif os.path.isfile(p):\n    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        zrok_token = f.read().strip()\n\nif not zrok_token:\n    print(\"❌ Token not found or empty:\", p)\n    sys.exit(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\nprint(\"Setting up models...\")\n\n# --- Copy Animagine-XL model if not already present ---\nsource = \"/kaggle/input/animagine-xl-3-1\"\ndest = \"/kaggle/working/cagliostrolab/animagine-xl-3.1\"\n\nif os.path.exists(dest):\n    print(f\"✓ Animagine-XL already exists at {dest}, skipping copy\")\nelse:\n    print(f\"  Copying Animagine-XL... (this may take a moment, it's large)\")\n    shutil.copytree(source, dest)\n    print(f\"  ✓ Copied to {dest}\")\n\n# --- Copy LCM-LoRA SDXL if not already present ---\nlora_source = \"/kaggle/input/lcm-lora-sdxl\"\nlora_dest = \"/kaggle/working/latent-consistency/lcm-lora-sdxl\"\n\nif os.path.exists(lora_dest):\n    print(f\"✓ LCM-LoRA SDXL already exists at {lora_dest}, skipping copy\")\nelse:\n    print(f\"  Copying LCM-LoRA SDXL...\")\n    shutil.copytree(lora_source, lora_dest)\n    print(f\"  ✓ Copied to {lora_dest}\")\n\nprint(\"✅ All models ready!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n\n# --- Device ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# --- Model / LoRA paths & IDs ---\nMODEL_ID = \"cagliostrolab/animagine-xl-3.1\"\nlocal_model_path = \"/kaggle/working/cagliostrolab/animagine-xl-3.1\"  # change to your working dir\n#LORA_ID = \"latent-consistency/lcm-lora-sdxl\"\n#local_lora_path = \"/kaggle/working/latent-consistency/lcm-lora-sdxl\"         # change to your working dir\n\n# --- Load or download Stable Diffusion (DiffusionPipeline) ---\nif os.path.exists(local_model_path):\n    pipe_txt2img = DiffusionPipeline.from_pretrained(\n        local_model_path,\n        #variant=\"fp16\",\n        torch_dtype=torch.float16\n    ).to(device)\nelse:\n    pipe_txt2img = DiffusionPipeline.from_pretrained(\n        MODEL_ID,\n        #variant=\"fp16\",\n        torch_dtype=torch.float16\n    ).to(device)\n    # save local copy\n    try:\n        pipe_txt2img.save_pretrained(local_model_path)\n    except Exception as e:\n        print(f\"Warning: failed to save pipeline locally to {local_model_path}: {e}\")\n\n# set scheduler\n#pipe_txt2img.scheduler = DPMSolverMultistepScheduler.from_config(pipe_txt2img.scheduler.config)\n\nfrom diffusers import DPMSolverMultistepScheduler\n\npipe_txt2img.scheduler = DPMSolverMultistepScheduler.from_config(\n    pipe_txt2img.scheduler.config,\n    algorithm_type=\"sde-dpmsolver++\"\n)\n\n\n# --- Load LCM-LoRA (prefer local copy) ---\n#if os.path.exists(local_lora_path):\n#    pipe_txt2img.load_lora_weights(local_lora_path)\n#else:\n#    pipe_txt2img.load_lora_weights(LORA_ID)\n\npipe_txt2img.safety_checker = lambda images, clip_input=None: (images, [False] * len(images))\n\n\n\n# ensure pipeline on device\n#pipe_txt2img = pipe_txt2img.to(device)","metadata":{"id":"6U5TrtW7hksb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"I7p5hf2ohk3z","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reusable text-to-image generator (returns PIL.Image for easy FastAPI integration)\nfrom datetime import datetime\nfrom IPython.display import display\nimport torch\nimport io\nimport os\n\ndef generate_txt2img(\n    pipe_txt2img,\n    prompt,\n    negative_prompt=None,\n    seed=None,\n    num_inference_steps=40,\n    guidance_scale=7.5,\n    height=768,\n    width=768,\n    device=None\n):\n    \"\"\"\n    Generate an image with `pipe_txt2img` and return a PIL.Image (does not save by default).\n    If save=True it will write a PNG to disk but still return the PIL.Image.\n    If return_metadata=True it returns (image, metadata_dict).\n    \"\"\"\n    # device detection\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # choose seed\n    if seed is None:\n        seed = int(torch.randint(0, 2**31 - 1, (1,)).item())\n\n    # create generator for the chosen device (fallback to CPU if unsupported)\n    try:\n        gen = torch.Generator(device=device).manual_seed(seed)\n    except Exception:\n        gen = torch.Generator().manual_seed(seed)\n\n    # build call kwargs\n    call_kwargs = {\n        \"prompt\": prompt,\n        \"num_inference_steps\": int(num_inference_steps),\n        \"guidance_scale\": float(guidance_scale),\n        \"generator\": gen,\n    }\n    if negative_prompt is not None:\n        call_kwargs[\"negative_prompt\"] = negative_prompt\n    if height is not None:\n        call_kwargs[\"height\"] = int(height)\n    if width is not None:\n        call_kwargs[\"width\"] = int(width)\n\n\n    # call the pipeline\n    pipeline_output = pipe_txt2img(**call_kwargs)\n\n    # extract image (typical diffusers output)\n    try:\n        image = pipeline_output.images[0]\n    except Exception:\n        image = pipeline_output[0]\n\n\n    return image","metadata":{"id":"CD7PUqlaYs3M","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"0PeqTDUPZ1Td","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"nOacxIfIYtTn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------\n# FastAPI app init\n# --------------------------\n\nfrom fastapi import FastAPI\nimport nest_asyncio\nimport uvicorn\n\n# Assume generate_txt2img, generate_img2img, pipe_txt2img, pipe_img2img are already defined\napp = FastAPI()\nnest_asyncio.apply()  # allow running uvicorn in Colab\n#print(\"FastAPI app initialized.\")","metadata":{"id":"NW9bWcz7Ytan","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"iQp-O3NNYtxG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# --------------------------# FastAPI route to expose generate_txt2img# --------------------------\nfrom fastapi import Form, HTTPException\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport io\nfrom datetime import datetime\n\n# NOTE:\n# - This cell expects `app`, `generate_txt2img`, and `pipe_txt2img` to already exist\n#   (as in your retained cell). It *does not* re-create or modify `app`.\n# - If you want to disable the safety checker globally, you could (in a separate\n#   cell) do something like:\n#   # pipe_txt2img.safety_checker = lambda images, clip_input=None: (images, [False] * len(images))\n#   # keep that commented here so you can toggle it manually.\n@app.post(\"/generate\")\nasync def generate_endpoint(\n    prompt: str = Form(...),\n    negative_prompt: str = Form(None),\n    seed: int = Form(None),\n    num_inference_steps: int = Form(40),\n    guidance_scale: float = Form(7.5),\n    height: int = Form(768),\n    width: int = Form(768)\n):\n\n    try:\n        # call your reusable function (uses pipe_txt2img already available)\n        image = generate_txt2img(\n            pipe_txt2img,\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            seed=seed,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            height=height,\n            width=width\n        )\n        # Convert PIL.Image to in-memory PNG\n        #buf = io.BytesIO()\n        #headers = {\"Content-Disposition\": f'inline; filename=\"{filename}\"'}\n        #return StreamingResponse(buf, media_type=\"image/png\", headers=headers)\n\n        # --- Return PNG stream ---\n        buffer = io.BytesIO()\n        image.save(buffer, format=\"PNG\")\n        buffer.seek(0)\n        return StreamingResponse(buffer, media_type=\"image/png\")\n\n\n    \n    except Exception as e:\n        # return a JSON error so the client can debug\n        return JSONResponse({\"error\": str(e)}, status_code=500)","metadata":{"id":"DB6l0kl2NpoT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"V08PrK5sNpyz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"DQKtR6TxO-Ts","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download zrok v1.1.3 (latest)\n!wget https://github.com/openziti/zrok/releases/download/v1.1.3/zrok_1.1.3_linux_amd64.tar.gz\n!tar -xzf zrok_1.1.3_linux_amd64.tar.gz\n!chmod +x zrok","metadata":{"id":"eO81fsViO-Ye","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enable (automatic migration from 0.4)\n!./zrok enable --headless \"$zrok_token\"\n\n# Use the agent for better process management\n#!./zrok agent start &\n#!./zrok share public localhost:8000 --headless","metadata":{"id":"3XGpCMjFO-bc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!./zrok disable","metadata":{"id":"qvl2xM3pO-d0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import uvicorn\nimport threading\n\ndef run_uvicorn():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n\n# Start in background thread\nthreading.Thread(target=run_uvicorn, daemon=True).start()","metadata":{"id":"pHH-93euO-ri","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import subprocess\nimport re\nimport time\n\ndef start_zrok_tunnel(port=8000):\n    # Start the tunnel\n    process = subprocess.Popen([\n        \"./zrok\", \"share\", \"public\", f\"localhost:{port}\", \"--headless\"\n    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\n    # Give it a moment to start\n    time.sleep(3)\n\n    # Check agent status to get the URL\n    status_process = subprocess.run([\n        \"./zrok\", \"agent\", \"status\"\n    ], capture_output=True, text=True)\n\n    print(\"Agent Status:\")\n    print(status_process.stdout)\n\n    return process\n\n# Start the tunnel\ntunnel_process = start_zrok_tunnel(8000)\nprint(\"Zrok tunnel started! Check the agent status above for your public URL.\")","metadata":{"id":"jWt8d9UqO-vU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"k5vT7HsOQIA7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!./zrok overview","metadata":{"id":"tAXFNNnpNqf7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"qqAO1R_kQIGK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"tZkjBnCpjyRF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nprint(\"Server and zrok tunnel are running. Keeping the notebook alive...\")\n\ntry:\n    while True:\n        time.sleep(60)\nexcept KeyboardInterrupt:\n    print(\"Shutting down.\")","metadata":{"id":"_vupHXeDjyVf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"2cHyLENtQIVw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"XsF3SWhNkqN_","trusted":true},"outputs":[],"execution_count":null}]}