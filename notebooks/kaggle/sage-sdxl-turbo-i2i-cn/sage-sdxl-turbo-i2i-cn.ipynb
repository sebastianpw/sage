{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION CELL\n",
    "# ---------------------------------------------------------------------------\n",
    "# Set to True to load the Img2Img pipeline, False for the Txt2Img pipeline.\n",
    "# This ensures we only use VRAM for one pipeline at a time.\n",
    "\n",
    "USE_IMG2IMG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "p = \"/kaggle/input/sage-zrok-token/.zrok_api_key\"\n",
    "zrok_token = None\n",
    "\n",
    "if os.path.isfile(p):\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        zrok_token = f.read().strip()\n",
    "\n",
    "if not zrok_token:\n",
    "    print(\"❌ Token not found or empty:\", p)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Setting up Models...\")\n",
    "\n",
    "# --- 1. Copy SDXL Turbo ---\n",
    "source_sd = \"/kaggle/input/sdxl-turbo\"\n",
    "dest_sd = \"/kaggle/working/stabilityai/sdxl-turbo\"\n",
    "\n",
    "if os.path.exists(dest_sd):\n",
    "    print(f\"✓ SDXL Turbo already exists at {dest_sd}\")\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(dest_sd), exist_ok=True)\n",
    "    print(f\"  Copying SDXL Turbo... (this may take a moment)\")\n",
    "    try:\n",
    "        shutil.copytree(source_sd, dest_sd)\n",
    "        print(f\"  ✓ Copied SDXL Turbo\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ Could not find SDXL Turbo dataset.\")\n",
    "\n",
    "# --- 2. Copy ControlNet OpenPose SDXL ---\n",
    "source_cn = \"/kaggle/input/controlnet-openpose-sdxl\"\n",
    "dest_cn = \"/kaggle/working/controlnet-openpose-sdxl\"\n",
    "\n",
    "if os.path.exists(dest_cn):\n",
    "    print(f\"✓ ControlNet OpenPose already exists at {dest_cn}\")\n",
    "else:\n",
    "    print(f\"  Copying ControlNet OpenPose...\")\n",
    "    try:\n",
    "        shutil.copytree(source_cn, dest_cn)\n",
    "        print(f\"  ✓ Copied ControlNet\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ ControlNet source not found at {source_cn}. Pipeline will fail if ControlNet is missing.\")\n",
    "\n",
    "print(\"✅ Models ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# LOAD PIPELINE WITH CONTROLNET\n",
    "# ---------------------------------------------------------------------------\n",
    "import torch\n",
    "from diffusers import (\n    ControlNetModel,\n    StableDiffusionXLControlNetPipeline,\n    StableDiffusionXLControlNetImg2ImgPipeline\n)\n",
    "\n",
    "# --- Paths ---\n",
    "MODEL_PATH = '/kaggle/working/stabilityai/sdxl-turbo'\n",
    "CONTROLNET_PATH = '/kaggle/working/controlnet-openpose-sdxl'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = None\n",
    "\n",
    "print(f\"Loading SDXL Turbo + ControlNet | Mode: {'IMG2IMG' if USE_IMG2IMG else 'TXT2IMG'}...\")\n",
    "\n",
    "# 1. Load ControlNet Model\n",
    "print(\"Loading ControlNet...\")\n",
    "try:\n",
    "    controlnet = ControlNetModel.from_pretrained(\n",
    "        CONTROLNET_PATH,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load ControlNet from {CONTROLNET_PATH}\")\n",
    "    raise e\n",
    "\n",
    "# 2. Load Main Pipeline\n",
    "if USE_IMG2IMG:\n",
    "    pipe = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        controlnet=controlnet,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "else:\n",
    "    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        controlnet=controlnet,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# 3. Speed Optimizations\n",
    "if hasattr(pipe, \"safety_checker\") and pipe.safety_checker is not None:\n",
    "    pipe.safety_checker = lambda images, clip_input=None: (images, [False] * len(images))\n",
    "\n",
    "print(\"✅ Pipeline loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# LONG PROMPT HANDLER (MANUAL CHUNKING - NO WARNINGS)\n",
    "# ------------------------------------------------------------------------------\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Suppress tokenizer warning\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "def get_long_prompt_embeddings(pipe, prompt, negative_prompt):\n",
    "    \"\"\"\n",
    "    Manually encodes prompts of ANY length for SDXL.\n",
    "    Generates 'prompt_embeds' (content) and 'pooled_prompt_embeds' (style).\n",
    "    \"\"\"\n",
    "    device = pipe.device\n",
    "    \n",
    "    def process_text(text):\n",
    "        tokenizers = [pipe.tokenizer, pipe.tokenizer_2]\n",
    "        text_encoders = [pipe.text_encoder, pipe.text_encoder_2]\n",
    "        embeds_list = []\n",
    "        pooled_embed = None\n",
    "\n",
    "        for i, (tokenizer, text_encoder) in enumerate(zip(tokenizers, text_encoders)):\n",
    "            input_ids = tokenizer(text, return_tensors=\"pt\", truncation=False).input_ids.to(device)\n",
    "            max_len = tokenizer.model_max_length\n",
    "            chunk_size = max_len - 2\n",
    "            \n",
    "            chunks = []\n",
    "            for k in range(0, input_ids.shape[-1], chunk_size):\n",
    "                chunk = input_ids[:, k:k + chunk_size]\n",
    "                bos = torch.tensor([tokenizer.bos_token_id], device=device).unsqueeze(0)\n",
    "                eos = torch.tensor([tokenizer.eos_token_id], device=device).unsqueeze(0)\n",
    "                chunk_padded = torch.cat([bos, chunk, eos], dim=1)\n",
    "                if chunk_padded.shape[-1] < max_len:\n",
    "                    pad = torch.full((1, max_len - chunk_padded.shape[-1]), tokenizer.pad_token_id, device=device)\n",
    "                    chunk_padded = torch.cat([chunk_padded, pad], dim=1)\n",
    "                chunk_padded = chunk_padded[:, :max_len]\n",
    "                chunks.append(chunk_padded)\n",
    "\n",
    "            layer_hidden_states = []\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                with torch.no_grad():\n",
    "                    output = text_encoder(chunk, output_hidden_states=True)\n",
    "                    layer_hidden_states.append(output.hidden_states[-2])\n",
    "                    if i == 1 and j == 0:\n",
    "                        pooled_embed = output.text_embeds\n",
    "\n",
    "            embeds_list.append(torch.cat(layer_hidden_states, dim=1))\n",
    "\n",
    "        len_1 = embeds_list[0].shape[1]\n",
    "        len_2 = embeds_list[1].shape[1]\n",
    "        min_len = min(len_1, len_2)\n",
    "        final_prompt_embeds = torch.cat([embeds_list[0][:, :min_len, :], embeds_list[1][:, :min_len, :]], dim=-1)\n",
    "        return final_prompt_embeds, pooled_embed\n",
    "\n",
    "    pos_embeds, pos_pooled = process_text(prompt)\n",
    "    neg_embeds, neg_pooled = process_text(negative_prompt)\n",
    "\n",
    "    p_len = pos_embeds.shape[1]\n",
    "    n_len = neg_embeds.shape[1]\n",
    "\n",
    "    if p_len > n_len:\n",
    "        pad = torch.zeros((1, p_len - n_len, neg_embeds.shape[-1]), device=device, dtype=neg_embeds.dtype)\n",
    "        neg_embeds = torch.cat([neg_embeds, pad], dim=1)\n",
    "    elif n_len > p_len:\n",
    "        neg_embeds = neg_embeds[:, :p_len, :]\n",
    "\n",
    "    return pos_embeds, neg_embeds, pos_pooled, neg_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# GENERATION FUNCTION\n",
    "# Handles Turbo, ControlNet + Resize Fix\n",
    "# -----------------------------\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def generate_sdxl_turbo(\n",
    "    pipe,\n",
    "    prompt: str,\n",
    "    negative_prompt: str = \"\",\n",
    "    init_image = None,\n",
    "    control_image = None,\n",
    "    strength: float = 0.5,\n",
    "    num_inference_steps: int = 2,\n",
    "    guidance_scale: float = 0.0,\n",
    "    controlnet_scale: float = 0.5,\n",
    "    seed: int = None,\n",
    "    height: int = 512,\n",
    "    width: int = 512\n",
    "):\n",
    "    device = pipe.device\n",
    "    if seed is None:\n",
    "        seed = int(torch.randint(0, 2**31 - 1, (1,)).item())\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    # 1. Get Long Prompt Embeddings\n",
    "    pos_emb, neg_emb, pos_pool, neg_pool = get_long_prompt_embeddings(pipe, prompt, negative_prompt)\n",
    "\n",
    "    # 2. Setup Dimensions & Control Image\n",
    "    # In Img2Img, target size is determined by init_image\n",
    "    target_w = init_image.width if init_image else width\n",
    "    target_h = init_image.height if init_image else height\n",
    "    \n",
    "    pipeline_control_image = control_image\n",
    "    effective_control_scale = float(controlnet_scale)\n",
    "\n",
    "    if pipeline_control_image is None:\n",
    "        # Dummy black image if no control provided\n",
    "        pipeline_control_image = Image.new(\"RGB\", (target_w, target_h), (0, 0, 0))\n",
    "        effective_control_scale = 0.0\n",
    "    \n",
    "    # [CRITICAL FIX] Resize ControlNet image to match Init Image exactly\n",
    "    # This prevents the \"tensor size mismatch (128 vs 64)\" error\n",
    "    if pipeline_control_image.size != (target_w, target_h):\n",
    "        pipeline_control_image = pipeline_control_image.resize((target_w, target_h), Image.LANCZOS)\n",
    "\n",
    "    # 3. Prepare Arguments\n",
    "    call_kwargs = {\n",
    "        \"prompt_embeds\": pos_emb,\n",
    "        \"pooled_prompt_embeds\": pos_pool,\n",
    "        \"negative_prompt_embeds\": neg_emb,\n",
    "        \"negative_pooled_prompt_embeds\": neg_pool,\n",
    "        \"num_inference_steps\": int(num_inference_steps),\n",
    "        \"guidance_scale\": float(guidance_scale),\n",
    "        \"generator\": generator,\n",
    "        \"controlnet_conditioning_scale\": effective_control_scale,\n",
    "    }\n",
    "\n",
    "    # 4. Mode Specific Arguments\n",
    "    if USE_IMG2IMG:\n",
    "        if init_image is None:\n",
    "            raise ValueError(\"Img2Img mode requires init_image.\")\n",
    "        call_kwargs[\"image\"] = init_image\n",
    "        call_kwargs[\"control_image\"] = pipeline_control_image\n",
    "        call_kwargs[\"strength\"] = float(strength)\n",
    "    else:\n",
    "        call_kwargs[\"image\"] = pipeline_control_image\n",
    "        call_kwargs[\"height\"] = int(height)\n",
    "        call_kwargs[\"width\"] = int(width)\n",
    "\n",
    "    # 5. Execute\n",
    "    with torch.inference_mode():\n",
    "        output = pipe(**call_kwargs)\n",
    "        image = output.images[0]\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# FastAPI app init\n",
    "# --------------------------\n",
    "from fastapi import FastAPI\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# SYNCHRONOUS ENDPOINT\n",
    "# ---------------------------\n",
    "from fastapi import Form, UploadFile, File\n",
    "from fastapi.responses import StreamingResponse, JSONResponse\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(\n",
    "    prompt: str = Form(...),\n",
    "    negative_prompt: str = Form(\"\"),\n",
    "    seed: int = Form(None),\n",
    "    num_inference_steps: int = Form(2),\n",
    "    guidance_scale: float = Form(0.0),\n",
    "    height: int = Form(512),\n",
    "    width: int = Form(512),\n",
    "    strength: float = Form(0.5),\n",
    "    controlnet_scale: float = Form(0.5),\n",
    "    init_image: UploadFile = File(None),\n",
    "    control_image: UploadFile = File(None),\n",
    "):\n",
    "    # --- Validation ---\n",
    "    if USE_IMG2IMG and init_image is None:\n",
    "        return JSONResponse(status_code=400, content={\"error\": \"Notebook is in Img2Img mode, but no init_image provided.\"})\n",
    "    if not USE_IMG2IMG and init_image is not None:\n",
    "        return JSONResponse(status_code=400, content={\"error\": \"Notebook is in Txt2Img mode, but init_image was provided.\"})\n",
    "\n",
    "    try:\n",
    "        # --- Read Images ---\n",
    "        pil_init = None\n",
    "        if init_image:\n",
    "            pil_init = Image.open(BytesIO(await init_image.read())).convert(\"RGB\")\n",
    "            \n",
    "        pil_control = None\n",
    "        if control_image:\n",
    "            pil_control = Image.open(BytesIO(await control_image.read())).convert(\"RGB\")\n",
    "\n",
    "        # --- Generate ---\n",
    "        generated_image = generate_sdxl_turbo(\n",
    "            pipe=pipe,\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            init_image=pil_init,\n",
    "            control_image=pil_control,\n",
    "            strength=strength,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            controlnet_scale=controlnet_scale,\n",
    "            seed=seed,\n",
    "            height=height,\n",
    "            width=width\n",
    "        )\n",
    "\n",
    "        # --- Return Stream ---\n",
    "        buffer = BytesIO()\n",
    "        generated_image.save(buffer, format=\"PNG\")\n",
    "        buffer.seek(0)\n",
    "        return StreamingResponse(buffer, media_type=\"image/png\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse(status_code=500, content={\"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download zrok v1.1.3 (latest)\n",
    "!wget https://github.com/openziti/zrok/releases/download/v1.1.3/zrok_1.1.3_linux_amd64.tar.gz\n",
    "!tar -xzf zrok_1.1.3_linux_amd64.tar.gz\n",
    "!chmod +x zrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enable (automatic migration from 0.4)\n",
    "!./zrok enable --headless \"$zrok_token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import threading\n",
    "\n",
    "def run_uvicorn():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "# Start in background thread\n",
    "threading.Thread(target=run_uvicorn, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "def start_zrok_tunnel(port=8000):\n",
    "    # Start the tunnel\n",
    "    process = subprocess.Popen([\n",
    "        \"./zrok\", \"share\", \"public\", f\"localhost:{port}\", \"--headless\"\n",
    "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Give it a moment to start\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Check agent status to get the URL\n",
    "    status_process = subprocess.run([\n",
    "        \"./zrok\", \"agent\", \"status\"\n",
    "    ], capture_output=True, text=True)\n",
    "\n",
    "    print(\"Agent Status:\")\n",
    "    print(status_process.stdout)\n",
    "\n",
    "    return process\n",
    "\n",
    "# Start the tunnel\n",
    "tunnel_process = start_zrok_tunnel(8000)\n",
    "print(\"Zrok tunnel started! Check the agent status above for your public URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Server and zrok tunnel are running. Keeping the notebook alive...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Shutting down.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8807372,
     "sourceId": 13924198,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8907734,
     "sourceId": 13972469,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}