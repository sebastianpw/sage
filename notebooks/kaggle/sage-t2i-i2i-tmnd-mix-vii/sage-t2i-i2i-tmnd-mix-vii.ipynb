{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "p = \"/kaggle/input/sage-zrok-token/.zrok_api_key\"\n",
    "zrok_token = None\n",
    "\n",
    "if os.path.isfile(p):\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        zrok_token = f.read().strip()\n",
    "\n",
    "if not zrok_token:\n",
    "    print(\"❌ Token not found or empty:\", p)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Setting up model...\")\n",
    "\n",
    "# --- Copy TMND-Mix VII if not already present ---\n",
    "source = \"/kaggle/input/tmnd-mix-vii\"\n",
    "dest = \"/kaggle/working/eienmojiki/tmnd-mix-vii\"\n",
    "\n",
    "if os.path.exists(dest):\n",
    "    print(f\"✓ TMND-Mix VII already exists at {dest}, skipping copy\")\n",
    "else:\n",
    "    print(f\"  Copying TMND-Mix VII...\")\n",
    "    shutil.copytree(source, dest)\n",
    "    print(f\"  ✓ Copied to {dest}\")\n",
    "\n",
    "# --- Copy ControlNet OpenPose if not already present ---\n",
    "controlnet_source = \"/kaggle/input/controlnet-openpose\"\n",
    "controlnet_dest = \"/kaggle/working/sd-controlnet-openpose\"\n",
    "\n",
    "if os.path.exists(controlnet_dest):\n",
    "    print(f\"✓ ControlNet already exists at {controlnet_dest}, skipping copy\")\n",
    "else:\n",
    "    print(f\"  Copying ControlNet OpenPose...\")\n",
    "    shutil.copytree(controlnet_source, controlnet_dest)\n",
    "    print(f\"  ✓ Copied to {controlnet_dest}\")\n",
    "\n",
    "print(\"✅ All models ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1616,
     "status": "ok",
     "timestamp": 1758021856811,
     "user": {
      "displayName": "Peter Sebring",
      "userId": "09649677370512595693"
     },
     "user_tz": -120
    },
    "id": "db95k6-29391",
    "outputId": "81205e8c-eaad-4451-8aea-412ccbb9b777"
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# TXT2IMG pipeline\n",
    "# -----------------\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DPMSolverMultistepScheduler\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# --- Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Model paths ---\n",
    "MODEL_ID = \"eienmojiki/tmnd-mix-vii\"\n",
    "CONTROLNET_ID = \"lllyasviel/sd-controlnet-openpose\"\n",
    "local_model_path = \"/kaggle/working/eienmojiki/tmnd-mix-vii\"\n",
    "local_controlnet_path = \"/kaggle/working/sd-controlnet-openpose\"\n",
    "\n",
    "# --- Load or download ControlNet ---\n",
    "if os.path.exists(local_controlnet_path):\n",
    "    controlnet = ControlNetModel.from_pretrained(local_controlnet_path, torch_dtype=torch.float16).to(device)\n",
    "else:\n",
    "    controlnet = ControlNetModel.from_pretrained(CONTROLNET_ID, torch_dtype=torch.float16).to(device)\n",
    "    controlnet.save_pretrained(local_controlnet_path)\n",
    "\n",
    "# --- Load or download Stable Diffusion pipeline ---\n",
    "if os.path.exists(local_model_path):\n",
    "    pipe_txt2img = StableDiffusionControlNetPipeline.from_pretrained(local_model_path, controlnet=controlnet, torch_dtype=torch.float16, safety_checker=None).to(device)\n",
    "else:\n",
    "    pipe_txt2img = StableDiffusionControlNetPipeline.from_pretrained(MODEL_ID, controlnet=controlnet, torch_dtype=torch.float16, safety_checker=None).to(device)\n",
    "    pipe_txt2img.save_pretrained(local_model_path)\n",
    "\n",
    "# --- Fast scheduler ---\n",
    "#pipe_txt2img.scheduler = DPMSolverMultistepScheduler.from_config(pipe_txt2img.scheduler.config)\n",
    "\n",
    "from diffusers import EulerAncestralDiscreteScheduler\n",
    "pipe_txt2img.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe_txt2img.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPf2-pJY9396"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# TXT2IMG method [handles optional CN]\n",
    "# -------------------------------------\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from typing import Optional\n",
    "import random\n",
    "\n",
    "def _round_to_multiple(x: int, base: int = 8) -> int:\n",
    "    return ((x + base - 1) // base) * base\n",
    "\n",
    "def generate_txt2img(\n",
    "    pipe_txt2img,\n",
    "    prompt: str,\n",
    "    negative_prompt: Optional[str] = None,\n",
    "    height: int = 512,\n",
    "    width: int = 512,\n",
    "    num_inference_steps: int = 35,\n",
    "    guidance_scale: float = 7.5,\n",
    "    seed: Optional[int] = None,\n",
    "    controlnet_image: Optional[Image.Image] = None,\n",
    "    controlnet_scale: float = 1.0,\n",
    "    use_controlnet: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate an image from text using a Stable Diffusion pipeline.\n",
    "    Can optionally use ControlNet with a conditioning image.\n",
    "    Returns a PIL.Image.\n",
    "    \"\"\"\n",
    "    print(\"Starting TXT2IMG generation...\")\n",
    "\n",
    "    # Round dims to a multiple of 8 (stable-diffusion-friendly)\n",
    "    height = _round_to_multiple(height, 8)\n",
    "    width  = _round_to_multiple(width, 8)\n",
    "\n",
    "    # Resize ControlNet conditioning image if provided\n",
    "    if use_controlnet and controlnet_image is not None:\n",
    "        controlnet_image = controlnet_image.resize((width, height))\n",
    "\n",
    "    # Always pass an image to avoid pipeline errors\n",
    "    pipeline_image = controlnet_image if use_controlnet else Image.new(\"RGB\", (width, height), (0, 0, 0))\n",
    "\n",
    "    # Create generator for reproducibility (if seed provided)\n",
    "    generator = None\n",
    "\n",
    "    # Generate random seed if none provided\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "\n",
    "    generator = torch.Generator(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    # Build kwargs for the pipeline call\n",
    "    kwargs = {\n",
    "        \"prompt\": prompt,\n",
    "        \"negative_prompt\": negative_prompt,\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"num_inference_steps\": num_inference_steps,\n",
    "        \"guidance_scale\": guidance_scale,\n",
    "        \"image\": pipeline_image,\n",
    "        \"controlnet_conditioning_scale\": controlnet_scale if use_controlnet else 0.0,\n",
    "    }\n",
    "\n",
    "    if generator is not None:\n",
    "        kwargs[\"generator\"] = generator\n",
    "\n",
    "    # Run generation with appropriate context managers\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\"):\n",
    "            output = pipe_txt2img(**kwargs)\n",
    "    else:\n",
    "        with torch.inference_mode():\n",
    "            output = pipe_txt2img(**kwargs)\n",
    "\n",
    "    print(\"TXT2IMG generation completed\")\n",
    "    return output.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3158,
     "status": "ok",
     "timestamp": 1758034889759,
     "user": {
      "displayName": "Peter Sebring",
      "userId": "09649677370512595693"
     },
     "user_tz": -120
    },
    "id": "zjslSBfB93-g",
    "outputId": "9b99459c-0210-4510-af62-8da3d0399edc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##### NU COMBINED PIPE IMG2IMG+CN\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# IMG2IMG + CN pipeline\n",
    "# ----------------------\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    StableDiffusionControlNetImg2ImgPipeline,\n",
    "    ControlNetModel,\n",
    "    DPMSolverMultistepScheduler\n",
    ")\n",
    "\n",
    "# --- Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Model paths ---\n",
    "MODEL_ID = \"eienmojiki/tmnd-mix-vii\"\n",
    "CONTROLNET_ID = \"lllyasviel/sd-controlnet-openpose\"\n",
    "local_model_path = \"/kaggle/working/eienmojiki/tmnd-mix-vii\"\n",
    "local_controlnet_path = \"/kaggle/working/sd-controlnet-openpose\"\n",
    "\n",
    "# ------------------ #\n",
    "# --- ControlNet --- #\n",
    "# ------------------ #\n",
    "# --- Load or download ControlNet ---\n",
    "if os.path.exists(local_controlnet_path):\n",
    "    controlnet = ControlNetModel.from_pretrained(local_controlnet_path, torch_dtype=torch.float16).to(device)\n",
    "else:\n",
    "    controlnet = ControlNetModel.from_pretrained(CONTROLNET_ID, torch_dtype=torch.float16).to(device)\n",
    "    controlnet.save_pretrained(local_controlnet_path)\n",
    "\n",
    "\n",
    "# ------------------------ #\n",
    "# --- Img2Img pipeline --- #\n",
    "# ------------------------ #\n",
    "if os.path.exists(local_model_path):\n",
    "    pipe_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        local_model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None\n",
    "    ).to(device)\n",
    "else:\n",
    "    pipe_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None\n",
    "    ).to(device)\n",
    "    pipe_img2img.save_pretrained(local_model_path)\n",
    "\n",
    "# --- Fast scheduler ---\n",
    "#pipe_img2img.scheduler = DPMSolverMultistepScheduler.from_config(pipe_img2img.scheduler.config)\n",
    "\n",
    "\n",
    "from diffusers import EulerAncestralDiscreteScheduler\n",
    "pipe_img2img.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe_img2img.scheduler.config)\n",
    "\n",
    "# --------------------------- #\n",
    "# --- Img2Img CN pipeline --- #\n",
    "# --------------------------- #\n",
    "if os.path.exists(local_model_path):\n",
    "    pipe_cn_img2img = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
    "        local_model_path,\n",
    "        controlnet=controlnet,\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None\n",
    "    ).to(device)\n",
    "else:\n",
    "    pipe_cn_img2img = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        controlnet=controlnet,\n",
    "        torch_dtype=torch.float16,\n",
    "        safety_checker=None\n",
    "    ).to(device)\n",
    "    pipe_cn_img2img.save_pretrained(local_model_path)\n",
    "\n",
    "# --- Fast scheduler ---\n",
    "#pipe_cn_img2img.scheduler = DPMSolverMultistepScheduler.from_config(pipe_cn_img2img.scheduler.config)\n",
    "\n",
    "\n",
    "from diffusers import EulerAncestralDiscreteScheduler\n",
    "pipe_cn_img2img.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe_cn_img2img.scheduler.config)\n",
    "\n",
    "\n",
    "# TODO: test and reimplement:\n",
    "#pipe_img2img.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "#pipe_cn_img2img.safety_checker = lambda images, **kwargs: (images, [False] * len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1758034904297,
     "user": {
      "displayName": "Peter Sebring",
      "userId": "09649677370512595693"
     },
     "user_tz": -120
    },
    "id": "IGkHFzbQ93-h"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------\n",
    "# img2img method [handles optional CN]\n",
    "# -------------------------------------\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def generate_img2img(\n",
    "    pipe_img2img,\n",
    "    pipe_cn_img2img,\n",
    "    prompt,\n",
    "    negative_prompt,\n",
    "    init_image,\n",
    "    control_image=None,\n",
    "    strength=0.8,\n",
    "    guidance_scale=8.0,\n",
    "    controlnet_conditioning_scale=1.0,\n",
    "    num_inference_steps=50,\n",
    "    seed=None,\n",
    "    dtype=torch.float16,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Unified Img2Img function:\n",
    "    - if control_image is None -> use plain Img2Img pipeline\n",
    "    - if control_image is provided -> use ControlNet Img2Img pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    if control_image is not None:\n",
    "        if control_image.size != init_image.size:\n",
    "            control_image = control_image.resize(init_image.size)\n",
    "\n",
    "    # Generate random seed if none provided\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "\n",
    "    # Create generator\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    # Ensure proper autocast context\n",
    "    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=dtype):\n",
    "        if control_image is None:\n",
    "            # Plain Img2Img\n",
    "            output = pipe_img2img(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=init_image,\n",
    "                strength=strength,\n",
    "                guidance_scale=guidance_scale,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                generator=generator,\n",
    "            )\n",
    "        else:\n",
    "            # ControlNet Img2Img\n",
    "            output = pipe_cn_img2img(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=init_image,\n",
    "                control_image=control_image,\n",
    "                strength=strength,\n",
    "                guidance_scale=guidance_scale,\n",
    "                controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                generator=generator,\n",
    "            )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96edX2PB9399"
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# FastAPI app init\n",
    "# --------------------------\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "# Assume generate_txt2img, generate_img2img, pipe_txt2img, pipe_img2img are already defined\n",
    "app = FastAPI()\n",
    "nest_asyncio.apply()  # allow running uvicorn in Colab\n",
    "#print(\"FastAPI app initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1758034917604,
     "user": {
      "displayName": "Peter Sebring",
      "userId": "09649677370512595693"
     },
     "user_tz": -120
    },
    "id": "ayi3rH4Q939_"
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# NEW TESTED T2I/I2I ENDPOINT /generate\n",
    "# ---------------------------\n",
    "\n",
    "from fastapi import FastAPI, Form, UploadFile, File\n",
    "from fastapi.responses import StreamingResponse, JSONResponse\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "img2img_inference_steps = 65\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def _parse_bool(val) -> bool:\n",
    "    if val is None:\n",
    "        return False\n",
    "    return str(val).strip().lower() in (\"1\", \"true\", \"yes\", \"y\", \"on\")\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(\n",
    "    prompt: str = Form(...),\n",
    "    negative_prompt: str = Form(None),\n",
    "    height: int = Form(768),\n",
    "    width: int = Form(768),\n",
    "    num_inference_steps: int = Form(65),\n",
    "    guidance_scale: float = Form(10.0),\n",
    "    seed: int = Form(None),\n",
    "\n",
    "    # img2img-only params\n",
    "    strength: float = Form(0.7),\n",
    "    init_image: UploadFile = File(None),  # optional, if present = img2img\n",
    "\n",
    "    # controlnet\n",
    "    use_controlnet: str = Form(\"0\"),\n",
    "    controlnet_scale: float = Form(1.0),\n",
    "    control_image: UploadFile = File(None),  # optional\n",
    "):\n",
    "    \"\"\"Unified endpoint: txt2img if no init_image, img2img if init_image provided.\"\"\"\n",
    "\n",
    "    # --- Load ControlNet image if present ---\n",
    "    controlnet_img = None\n",
    "    if control_image is not None:\n",
    "        try:\n",
    "            controlnet_img = Image.open(\n",
    "                BytesIO(await control_image.read())\n",
    "            ).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            return JSONResponse(\n",
    "                status_code=400,\n",
    "                content={\"error\": f\"Failed to read control_image: {str(e)}\"},\n",
    "            )\n",
    "\n",
    "    use_controlnet_flag = _parse_bool(use_controlnet) or controlnet_img is not None\n",
    "\n",
    "    # --- Img2img path ---\n",
    "    if init_image is not None:\n",
    "        try:\n",
    "            init_img = Image.open(BytesIO(await init_image.read())).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            return JSONResponse(\n",
    "                status_code=400,\n",
    "                content={\"error\": f\"Failed to read init_image: {str(e)}\"},\n",
    "            )\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # -----------------------------\n",
    "            # Run unified Img2Img method\n",
    "            # -----------------------------\n",
    "\n",
    "\n",
    "\n",
    "            generated_images = generate_img2img(\n",
    "                pipe_img2img=pipe_img2img,\n",
    "                pipe_cn_img2img=pipe_cn_img2img,\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                init_image=init_img,\n",
    "                control_image=controlnet_img,\n",
    "                strength=strength,\n",
    "                guidance_scale=guidance_scale,\n",
    "                controlnet_conditioning_scale=1.8,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                seed=seed\n",
    "            )\n",
    "\n",
    "            generated_image = generated_images.images[0]\n",
    "\n",
    "\n",
    "\n",
    "                #height=height,\n",
    "                #width=width,\n",
    "                #num_inference_steps=num_inference_steps,\n",
    "\n",
    "                #seed=seed,\n",
    "\n",
    "                #controlnet_scale=controlnet_scale,\n",
    "                #use_controlnet=use_controlnet_flag,\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            return JSONResponse(status_code=500, content={\"error\": str(e)})\n",
    "\n",
    "    # --- Txt2img path ---\n",
    "    else:\n",
    "        try:\n",
    "            generated_image = generate_txt2img(\n",
    "                pipe_txt2img=pipe_txt2img,\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                seed=seed,\n",
    "                controlnet_image=controlnet_img,\n",
    "                controlnet_scale=controlnet_scale,\n",
    "                use_controlnet=use_controlnet_flag,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return JSONResponse(status_code=500, content={\"error\": str(e)})\n",
    "\n",
    "    # --- Return PNG stream ---\n",
    "    buffer = BytesIO()\n",
    "    generated_image.save(buffer, format=\"PNG\")\n",
    "    buffer.seek(0)\n",
    "    return StreamingResponse(buffer, media_type=\"image/png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download zrok v1.1.3 (latest)\n",
    "!wget https://github.com/openziti/zrok/releases/download/v1.1.3/zrok_1.1.3_linux_amd64.tar.gz\n",
    "!tar -xzf zrok_1.1.3_linux_amd64.tar.gz\n",
    "!chmod +x zrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable (automatic migration from 0.4)\n",
    "!./zrok enable --headless \"$zrok_token\"\n",
    "\n",
    "# Use the agent for better process management\n",
    "#!./zrok agent start &\n",
    "#!./zrok share public localhost:8000 --headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!./zrok disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import threading\n",
    "\n",
    "def run_uvicorn():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "# Start in background thread\n",
    "threading.Thread(target=run_uvicorn, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "import time\n",
    "\n",
    "def start_zrok_tunnel(port=8000):\n",
    "    # Start the tunnel\n",
    "    process = subprocess.Popen([\n",
    "        \"./zrok\", \"share\", \"public\", f\"localhost:{port}\", \"--headless\"\n",
    "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Give it a moment to start\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Check agent status to get the URL\n",
    "    status_process = subprocess.run([\n",
    "        \"./zrok\", \"agent\", \"status\"\n",
    "    ], capture_output=True, text=True)\n",
    "\n",
    "    print(\"Agent Status:\")\n",
    "    print(status_process.stdout)\n",
    "\n",
    "    return process\n",
    "\n",
    "# Start the tunnel\n",
    "tunnel_process = start_zrok_tunnel(8000)\n",
    "print(\"Zrok tunnel started! Check the agent status above for your public URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./zrok overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Server and zrok tunnel are running. Keeping the notebook alive...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Shutting down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8550614,
     "sourceId": 13469709,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8899441,
     "sourceId": 13961012,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
